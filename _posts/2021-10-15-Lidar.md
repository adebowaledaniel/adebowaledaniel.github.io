---
title: "Lidar Classification - CloudCompare"
date: 2021-10-16
excerpt: "Point Cloud classification. <br/><img src='/images/Q.png'/>"
# permalink: /posts/2021/10/Lidar/
tags:
  - UBS
  - Active and Passive Remote Sensing
---

Overview
===
The objective of this exercise is to perform an advanced operation in [CloudCompare](https://www.danielgm.net/cc/), a 3D point cloud and mesh processing software
open-source software; the goal is to perform a binary semantic classification using multiscale dimensionality.  

Study Area and Data
===
The area of interest is Queens Towns, New Zealand; the AOI's choice was based on data availability.   
<img src="/images/Study-Area.png" alt="Study Area" style="height: 300px; width:500px;"/>  
Queenstown, New Zealand.

The point cloud data (Queenstown 2016) used is obtained from the OpenTopography; the data and its description can be openly accessed [here](https://portal.opentopography.org/lidarDataset?opentopoID=OTLAS.012019.2193.1). The point cloud data already came with two classifications, Ground(Brown) and Unclassified(Yellow), as seen in the figure below. A closer observation of the unclassified shows that it includes vegetation and infrastructure (for example, building and Bridge); therefore, in this exercise, a binary semantic classification for vegetation and buildings is executed using the CANUPO plugin in CloudCompare.  
<img src="/images/qclass.png" alt="Study Area" style="height: 300px; width:500px;"/>  

Methodology
====
To achieve the goal of this exercise, the *Unclassified* was separated from the *Ground* using the **Classification Scalar Values**.  

<img src="/images/unclassified.png" alt="Study Area" style="height: 300px; width:500px;"/>  

**Train Sample Selection**  
As aforementioned, this binary classification is aimed to further semantically separate the unclassified into Vegetation and Buildings. The sample selection process is crucial as it involves a logical selection of point clouds regarding their semantic representation in 3D of the two target classes. One of the precautions taken was to avoid the target classes' noise/point cloud mixture. For example, Figures 1 & 2 represent samples for vegetation and building selected, respectively; they were observed from a different perspective to ensure a possible mixture. Afterwards, various samples for each class are merged.  

<img src="/images/veg sample.png" alt="Study Area"/>  
Vegetation Sample  
<img src="/images/building sample.png" alt="Study Area" style="height: 100%; width:500px;"/>  
Building Sample   

**Training and Classification**
As a convention in open source software, the qCanupo plugin has been implemented in CloudComapre to handle the training and classification of point cloud data, [read more](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016GL072070) about its usage in exiting publication. The training samples for each class is evaluated with different metrics in CANUPO based on linear discriminant and distance to the boundary between the two clusters; the balanced accuracy and the [Fisher Discriminant Ratio (FDR)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) metric are considered for the evaluation as provided by qCanupo. A balanced accuracy of 0.95525 and an FDR of 4.434 are obtained for the study, setting the scale to *10*.  
<img src="/images/Stats.png" alt="Statistics" style="height: 100%; width:500px;"/>  
Evaluation metrics

Result
===
Below is the binary classification (Vegetation and Building) combined with the Ground point cloud.  
<img src="/images/Result1.png" alt="Statistics" style="height: 100%; width:500px;"/>  
  

<img src="/images/res2.png" alt="Statistics" style="height: 100%; width:90%;"/>  

